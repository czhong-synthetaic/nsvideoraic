{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from itertools import product\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import PIL\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import distance\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import matplotlib.pyplot as plt\n",
    "# !export CUDA_VISIBLE_DEVICES='0,1'\n",
    "# !pip install azure-storage-blob --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "# vits8 = torch.hub.load('facebookresearch/dino:main', 'dino_vits8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cca92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from itertools import product\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import PIL\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import distance\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import matplotlib.pyplot as plt\n",
    "# !export CUDA_VISIBLE_DEVICES='0,1'\n",
    "\n",
    "class DFDataset(Dataset):\n",
    "    def __init__(self, path, transform=None, overlap=128):\n",
    "        self.path = path\n",
    "        self.training_df = pd.read_parquet(self.path)\n",
    "        print('Training Samples', len(self.training_df))\n",
    "        self.training_frames = self.training_df.frames.tolist()\n",
    "        self.training_imgs = self.training_df.path.tolist()\n",
    "        self.length = len(self.training_imgs)\n",
    "\n",
    "        \n",
    "        self.overlap = overlap\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    # Try to turn this into torchvision.transforms so we can use with any dataset and so the input would be one image, and out put would be a flattented list of tiled images.\n",
    "    def get_patch(self, image, img_file, pid, frame, x, y, save=False):\n",
    "        #with an 800x600 pixel image, the image's left upper point is (0, 0), the right lower point is (800, 600).\n",
    "        xx = x + 256\n",
    "        yy = y + 256\n",
    "        \n",
    "        \n",
    "        patch = image.crop((x, y, xx, yy))\n",
    "        if save:\n",
    "            data_root = 'subtile'\n",
    "            os.makedir(os.path.join(data_root,frame), recursive=True, exists_ok=True)\n",
    "            patch.save(os.path.join(data_root, frame, f'{pid}.png'))\n",
    "            \n",
    "        patch_df = {'path': img_file, 'frame': frame, 'pid': pid, 'x1': x, 'y1': y, 'x2': xx, 'y2': yy }\n",
    "\n",
    "        \n",
    "        return (patch, patch_df)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        img_file = self.training_imgs[index]\n",
    "        img_file = '/data' + img_file\n",
    "        \n",
    "        image = PIL.Image.open(img_file).convert(\"RGB\")\n",
    "\n",
    "        wx, hy = image.size\n",
    "        \n",
    "        perm = product(\n",
    "            np.arange(0, wx, self.overlap).astype(int),\n",
    "            np.arange(0, hy, self.overlap).astype(int))\n",
    "        \n",
    "        frame = self.training_frames[index]\n",
    "        ret = [self.get_patch(image, img_file, idx, frame, x, y) for idx, (x, y) in enumerate(perm)]\n",
    "\n",
    "        imgs = [s[0] for s in ret]\n",
    "        imdf = [s[1] for s in ret]\n",
    "        \n",
    "        imdf = [(i) for i in imdf]\n",
    "        \n",
    "#         imdf = [pd.DataFrame(adf) for adf in imdf]\n",
    "        \n",
    "        imgs = [self.transform(animage) for animage in imgs]\n",
    "\n",
    "    \n",
    "        return imgs, imdf\n",
    "\n",
    "    \n",
    "def customcollate(batch):\n",
    "    '''\n",
    "    batch: the batch of data used in pytorch data loader class.\n",
    "    '''\n",
    "    imgs = [k[0] for k in batch]\n",
    "    imdf = [k[1] for k in batch]\n",
    "    all_images = []\n",
    "    \n",
    "    for aimg in imgs:\n",
    "        all_images.extend(aimg)\n",
    "    \n",
    "    all_dfs = []\n",
    "    for adf in imdf:\n",
    "        all_dfs.extend(adf)\n",
    "    \n",
    "    #     imdf = [s for s in imdf]\n",
    "#     imdf = pd.concat(imdf).reset_index(drop=True)\n",
    "    sample = torch.stack(all_images)\n",
    "        \n",
    "    return sample, all_dfs\n",
    "\n",
    "training_file = '/data/data2/dev/af3c9242-b676-499a-8910-65addbddb8da.parquet'\n",
    "# df = pd.read_parquet(training_file)\n",
    "\n",
    "\n",
    "txf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "dtset = DFDataset(training_file, txf)\n",
    "\n",
    "dtloader = torch.utils.data.DataLoader(\n",
    "    dtset,\n",
    "    batch_size=5,\n",
    "    collate_fn=customcollate,\n",
    "    num_workers=5,\n",
    ")\n",
    "\n",
    "sdf = []\n",
    "vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "\n",
    "vits16 = torch.nn.DataParallel(vits16)\n",
    "\n",
    "device = 'cuda'\n",
    "vits16.to(device)\n",
    "vits16.eval()\n",
    "\n",
    "avgpool = False\n",
    "\n",
    "\n",
    "def save_npy(filename, lv):\n",
    "    with open(filename, 'wb') as npf:\n",
    "        np.save(npf, lv)\n",
    "    return 1\n",
    "\n",
    "np_save = {}\n",
    "\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for idx, (imgs, imdfs) in enumerate(tqdm(dtloader)):\n",
    "        imgs = imgs.to(device)\n",
    "                \n",
    "        intermediate_output = vits16.module.get_intermediate_layers(imgs, 6)\n",
    "        output = [x[:, 0] for x in intermediate_output]\n",
    "        \n",
    "#         if avgpool:\n",
    "#             output.append(torch.mean(intermediate_output[-1][:, 1:], dim=1))\n",
    "        output = torch.cat(output, dim=-1)\n",
    "    \n",
    "        # NPY List\n",
    "        npimgs = output.detach().cpu().numpy().tolist()\n",
    "        cc = (imdfs[0]['path']).split('/')[-3]\n",
    "        frame = (imdfs[0]['frame'])\n",
    "        \n",
    "        save_dir='/data/data2/dev/nsvideoraic/output/'\n",
    "        meta_save_dir='/data/data2/dev/nsvideoraic/meta/'\n",
    "        \n",
    "        uniquef = []\n",
    "        for s, npar in zip(imdfs, npimgs):\n",
    "            s['cuid'] = cc\n",
    "            f1 = s['frame']\n",
    "            uniquef.append(f1)\n",
    "            \n",
    "            x1 = s['x1']\n",
    "            y1 = s['y1']\n",
    "            \n",
    "            npy_savedir = os.path.join(save_dir, str(cc), str(f1), str(x1))\n",
    "\n",
    "            meta_save = os.path.join(meta_save_dir, str(cc))\n",
    "            os.makedirs(npy_savedir, exist_ok=True)\n",
    "            os.makedirs(meta_save, exist_ok=True)\n",
    "            \n",
    "            npy_save = os.path.join(save_dir, str(cc), str(f1), str(x1), f'{str(y1)}.npy')\n",
    "\n",
    "            s['npyfile'] = npy_save \n",
    "            np_save[npy_save] = npar \n",
    "\n",
    "# \n",
    "        # Add NPY dest to imdfs\n",
    "        meta_save = os.path.join(meta_save_dir, str(cc))\n",
    "    \n",
    "        # Batch size must be 1, so that the whole batch is just one frame. Else it will store multiple\n",
    "        all_df = pd.DataFrame(imdfs)\n",
    "\n",
    "        for fff in uniquef:\n",
    "            aframedf = all_df[all_df['frame'] == fff]\n",
    "            aframedf.to_parquet(os.path.join(meta_save, f'frame{str(fff)}.parquet' ))\n",
    "\n",
    "                \n",
    "\n",
    "#         sdf.append(all_df)\n",
    "res = [save_npy(ff, ll) for (ff, ll) in (np_save.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda4c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install itertool\n",
    "from ns_preprocess import NSDataPreprocessing\n",
    "import pandas as pd\n",
    "# !pip install itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d7663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Using BiT-M-R101x1 as RAIC model.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples 857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1153.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "100%|██████████| 3/3 [00:04<00:00,  1.58s/it]\n"
     ]
    }
   ],
   "source": [
    "training_file = '/data/data2/dev/rest/17bbd414-4132-464a-bc1e-ab5dd66c7e8f.parquet'\n",
    "indf = pd.read_parquet(training_file)\n",
    "s = NSDataPreprocessing(indf)\n",
    "s.calculate_raic_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd53020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  path frame  pid   x1   y1  \\\n",
      "0    /data/data2/datasets/video/17bbd414-4132-464a-...   805    0    0    0   \n",
      "1    /data/data2/datasets/video/17bbd414-4132-464a-...   805    1    0  128   \n",
      "2    /data/data2/datasets/video/17bbd414-4132-464a-...   805    2    0  256   \n",
      "3    /data/data2/datasets/video/17bbd414-4132-464a-...   805    3    0  384   \n",
      "4    /data/data2/datasets/video/17bbd414-4132-464a-...   805    4    0  512   \n",
      "..                                                 ...   ...  ...  ...  ...   \n",
      "395  /data/data2/datasets/video/17bbd414-4132-464a-...    27   35  896    0   \n",
      "396  /data/data2/datasets/video/17bbd414-4132-464a-...    27   36  896  128   \n",
      "397  /data/data2/datasets/video/17bbd414-4132-464a-...    27   37  896  256   \n",
      "398  /data/data2/datasets/video/17bbd414-4132-464a-...    27   38  896  384   \n",
      "399  /data/data2/datasets/video/17bbd414-4132-464a-...    27   39  896  512   \n",
      "\n",
      "       x2   y2                                            npypath  \n",
      "0     256  256  /data/data2/datasets/video/video-display/laten...  \n",
      "1     256  384  /data/data2/datasets/video/video-display/laten...  \n",
      "2     256  512  /data/data2/datasets/video/video-display/laten...  \n",
      "3     256  640  /data/data2/datasets/video/video-display/laten...  \n",
      "4     256  768  /data/data2/datasets/video/video-display/laten...  \n",
      "..    ...  ...                                                ...  \n",
      "395  1152  256  /data/data2/datasets/video/video-display/laten...  \n",
      "396  1152  384  /data/data2/datasets/video/video-display/laten...  \n",
      "397  1152  512  /data/data2/datasets/video/video-display/laten...  \n",
      "398  1152  640  /data/data2/datasets/video/video-display/laten...  \n",
      "399  1152  768  /data/data2/datasets/video/video-display/laten...  \n",
      "\n",
      "[400 rows x 8 columns]\n",
      "Example file: /data/data2/datasets/video/17bbd414-4132-464a-bc1e-ab5dd66c7e8f/high/frame_805.jpg\n"
     ]
    }
   ],
   "source": [
    "s.write_latent_vector_arrays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5f538cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.finalize_save_meta_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2218a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = '/data/data2/dev/af3c9242-b676-499a-8910-65addbddb8da.parquet'\n",
    "\n",
    "s.get_vectors(indf=training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = '/data/data2/dev/af3c9242-b676-499a-8910-65addbddb8da.parquet'\n",
    "# df = pd.read_parquet(training_file)\n",
    "\n",
    "\n",
    "txf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "dtset = DFDataset(training_file, txf)\n",
    "\n",
    "dtloader = torch.utils.data.DataLoader(\n",
    "    dtset,\n",
    "    batch_size=1,\n",
    "    collate_fn=customcollate,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "sdf = []\n",
    "\n",
    "# vits16 = torch.nn.DataParallel(vits16)\n",
    "\n",
    "device = 'cuda'\n",
    "vits16.to(device)\n",
    "vits16.eval()\n",
    "\n",
    "avgpool = False\n",
    "\n",
    "# def save_npy(curdf, npy_file, save_dir, meta_save_dir):\n",
    "#     # Create save dest directories\n",
    "#     info = curdf['path'].split('/')\n",
    "#     cf = curdf['frame']\n",
    "#     cx = curdf['x1']\n",
    "#     cy = curdf['y1']\n",
    "#     cc = (info[-3]).split(\".\")[0]\n",
    "\n",
    "#     x_dir = os.path.join(save_dir, cc, str(cf), str(cx))\n",
    "#     if not os.path.exists(x_dir):\n",
    "#         os.makedirs(x_dir, exist_ok=True)\n",
    "#     meta_dir = os.path.join(meta_save_dir, cc)\n",
    "#     if not os.path.exists(meta_dir):\n",
    "#         os.makedirs(meta_dir, exist_ok=True)\n",
    "\n",
    "#     npy_save = os.path.join(save_dir, cc, str(cf), str(cx), f'{str(cy)}.npy')\n",
    "    \n",
    "#     with open(npy_save, 'wb') as f:\n",
    "#         np.save(f, npy_file)\n",
    "    \n",
    "#     curdf['npyfile'] = npy_save\n",
    "    \n",
    "#     return curdf\n",
    "            \n",
    "    \n",
    "np_save = {}\n",
    "\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for idx, (imgs, imdfs) in enumerate(tqdm(dtloader)):\n",
    "        imgs = imgs.to(device)\n",
    "                \n",
    "        intermediate_output = vits16.get_intermediate_layers(imgs, 6)\n",
    "        output = [x[:, 0] for x in intermediate_output]\n",
    "        \n",
    "#         if avgpool:\n",
    "#             output.append(torch.mean(intermediate_output[-1][:, 1:], dim=1))\n",
    "        output = torch.cat(output, dim=-1)\n",
    "    \n",
    "        # NPY List\n",
    "        npimgs = output.detach().cpu().numpy().tolist()\n",
    "        cc = (imdfs[0]['path']).split('/')[-3]\n",
    "        frame = (imdfs[0]['frame'])\n",
    "        \n",
    "        save_dir='/data/data2/dev/nsvideoraic/output/'\n",
    "        meta_save_dir='/data/data2/dev/nsvideoraic/meta/'\n",
    "        \n",
    "        for s, npar in zip(imdfs, npimgs):\n",
    "            s['cuid'] = cc\n",
    "            f1 = s['frame']\n",
    "            x1 = s['x1']\n",
    "            y1 = s['y1']\n",
    "            \n",
    "            npy_savedir = os.path.join(save_dir, str(cc), str(f1), str(x1))\n",
    "\n",
    "            meta_save = os.path.join(meta_save_dir, str(cc))\n",
    "            os.makedirs(npy_savedir, exist_ok=True)\n",
    "            os.makedirs(meta_save, exist_ok=True)\n",
    "            \n",
    "            npy_save = os.path.join(save_dir, str(cc), str(f1), str(x1), f'{str(y1)}.npy')\n",
    "\n",
    "            s['npyfile'] = npy_save \n",
    "            np_save[npy_save] = npar \n",
    "\n",
    "\n",
    "        # DF List\n",
    "        \n",
    "        # Determine Save directories\n",
    "#         save_dir = self.save_dir\n",
    "#         meta_save_dir = selmeta_save_dirve_dir\n",
    "\n",
    "#         lc = [save_npy(npysave, npyarray, save_dir, meta_save_dir) for npysave, npyarray in zip(imdfs, npimgs)]\n",
    "# \n",
    "        # Add NPY dest to imdfs\n",
    "        meta_save = os.path.join(meta_save_dir, str(cc))\n",
    "    \n",
    "        all_df = pd.DataFrame(imdfs)\n",
    "        all_df.to_parquet(os.path.join(meta_save, f'frame{str(frame)}.parquet' ))\n",
    "#         sdf.append(all_df)\n",
    "        \n",
    "# all_df = pd.concat(sdf, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "# all_df.to_csv('./infernce.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131af236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_npy(filename, lv):\n",
    "    with open(filename, 'wb') as npf:\n",
    "        np.save(npf, lv)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc44043",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [save_npy(ff, ll) for (ff, ll) in (np_save.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113a942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e7af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from itertools import product\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import PIL\n",
    "import os\n",
    "from scipy.spatial import distance\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import matplotlib.pyplot as plt\n",
    "import ray \n",
    "# ray.init()\n",
    "\n",
    "# @ray.remote\n",
    "def save_npy(npy_save, npy_file):\n",
    "    with open(npy_save, 'wb') as f:\n",
    "        np.save(f, npy_file)\n",
    "           \n",
    "\n",
    "class NSMain():\n",
    "    def __init__(self, model, save_dir='/data/data2/dev/nsvideoraic/output/', meta_save_dir='/data/data2/dev/nsvideoraic/meta/', vit=True):\n",
    "        super(NSMain, self).__init__()\n",
    "        \n",
    "        # Init multiprocessing RAY\n",
    "        \n",
    "        self.save_dir = save_dir\n",
    "        self.meta_save_dir = meta_save_dir\n",
    "        self.model = model\n",
    "        self.vit = vit\n",
    "        \n",
    "\n",
    "        self.device = 'cuda'\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.batch_size = 4\n",
    "    \n",
    " \n",
    "\n",
    "    def process_all_frame(self, all_df_file):\n",
    "        dfs = []\n",
    "        \n",
    "        txf = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "\n",
    "        dtset = DFDataset(all_df_file, txf)\n",
    "\n",
    "        dtloader = torch.utils.data.DataLoader(\n",
    "            dtset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=customcollate,\n",
    "            num_workers=6,\n",
    "        )  \n",
    "                \n",
    "        with torch.no_grad():\n",
    "            for idx, (imgs, imdfs) in enumerate(dtloader):\n",
    "                imgs = imgs.to(self.device)\n",
    "                batched_paths = imdfs.fimage\n",
    "\n",
    "                if self.vit:\n",
    "                    intermediate_output = vits16.get_intermediate_layers(imgs, 6)\n",
    "                    output = [x[:, 0] for x in intermediate_output]\n",
    "                    output = torch.cat(output, dim=-1)\n",
    "                else:\n",
    "                    output = model(imgs)\n",
    "                    \n",
    "                # Write each of these to Disks\n",
    "                npimgs = output.detach().cpu().numpy().tolist()\n",
    "                all_infos = [tmp.split('/') for tmp in batched_paths]\n",
    "                all_cuid = [tmp[-3].split(\".\")[0] for tmp in all_infos]\n",
    "#                 imdfs['cuid'] = all_cuid\n",
    "                \n",
    "                frame = imdfs.frame\n",
    "                x = imdfs.x1\n",
    "                y = imdfs.y1\n",
    "                \n",
    "                # Create save dest directories\n",
    "                for cc, cf, cx in zip(all_cuid, frame, x):\n",
    "                    x_dir = os.path.join(self.save_dir, cc, str(cf), str(cx))\n",
    "                    if not os.path.exists(x_dir):\n",
    "                        os.makedirs(x_dir, exist_ok=True)\n",
    "                    meta_dir = os.path.join(self.meta_save_dir, cc)\n",
    "                    if not os.path.exists(meta_dir):\n",
    "                        os.makedirs(meta_dir, exist_ok=True)\n",
    "                    \n",
    "                # Create NPY file paths\n",
    "                np_save_paths = [os.path.join(self.save_dir, cc, str(cf), str(cx), f'{str(cy)}.npy') for (cc, cf, cx, cy) in zip(all_cuid, frame, x, y)]\n",
    "                \n",
    "                # Save NPY files using RAY\n",
    "                lc = [save_npy( npysave, npyarray) for (npysave, npyarray) in zip(np_save_paths, npimgs)]\n",
    "            \n",
    "                # Add NPY dest to imdfs\n",
    "                imdfs['npypath'] = np_save_paths\n",
    "                \n",
    "#                 meta_data_save = []\n",
    "                # Split into uuids\n",
    "#                 gk = imdfs.groupby(\"cuid\")\n",
    "                \n",
    "#                 for \n",
    "                # Create Meta data directory\n",
    "#                 unique_meta_ids = imdfs.f\n",
    "                \n",
    "                dfs.append(imdfs)\n",
    "        \n",
    "        dfs = pd.concat(dfs)\n",
    "#                 imdfs.to_csv(os.path.join(meta_datas_frame, f'metadata.csv'))\n",
    "#         dfs.to_parquet('./allframes.parquet')\n",
    "        dfs.to_csv(\"/data/data2/dev/allframes.csv\")\n",
    "    \n",
    "    def process_one_frame(self, ondf_file:str):\n",
    "        dfs = []\n",
    "        \n",
    "        txf = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "\n",
    "        dtset = DFDataset(ondf_file, txf)\n",
    "\n",
    "        dtloader = torch.utils.data.DataLoader(\n",
    "            dtset,\n",
    "            batch_size=1,\n",
    "            collate_fn=customcollate,\n",
    "            num_workers=6,\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx, (imgs, imdfs) in enumerate(dtloader):\n",
    "                imgs = imgs.to(device)\n",
    "                batched_paths = imdfs.fimage\n",
    "\n",
    "                if self.vit:\n",
    "                    intermediate_output = vits16.get_intermediate_layers(imgs, 6)\n",
    "                    output = [x[:, 0] for x in intermediate_output]\n",
    "                    output = torch.cat(output, dim=-1)\n",
    "                else:\n",
    "                    output = model(imgs)\n",
    "                    \n",
    "                # Write each of these to Disks\n",
    "                npimgs = output.detach().cpu().numpy().tolist()\n",
    "                all_infos = [tmp.split('/') for tmp in batched_paths]\n",
    "                all_cuid = [tmp[-3].split(\".\")[0] for tmp in all_infos]\n",
    "                imdfs['cuid'] = all_cuid\n",
    "                \n",
    "                frame = imdfs.frames\n",
    "                x = imdfs.x1\n",
    "                y = imdfs.y1\n",
    "                \n",
    "                # Create save dest directories\n",
    "                for cc, cf, cx in zip(all_cuid, frame, x):\n",
    "                    x_dir = os.path.join(self.save_dir, cc, str(cf), str(cx))\n",
    "                    if not os.path.exists(x_dir):\n",
    "                        os.makedirs(x_dir, exist_ok=True)\n",
    "                    meta_dir = os.path.join(self.meta_save_dir, cc)\n",
    "                    if not os.path.exists(meta_dir):\n",
    "                        os.makedirs(meta_dir, exist_ok=True)\n",
    "                    \n",
    "                # Create NPY file paths\n",
    "                np_save_paths = [os.path.join(self.save_dir, cc, str(cf), str(cx), f'{str(cy)}.npy')\n",
    "                                 for (cc, cf, cx, cy) in zip(all_cuid, frame, x, y)]\n",
    "                \n",
    "                # Save NPY files using RAY\n",
    "                lc = [save_npy.remote(npysave, npyarray) for npysave, npyarray in zip(np_save_paths, npimgs)]\n",
    "            \n",
    "                # Add NPY dest to imdfs\n",
    "                imdfs['npypath'] = np_save_paths\n",
    "                \n",
    "#                 meta_data_save = []\n",
    "                # Split into uuids\n",
    "#                 gk = imdfs.groupby(\"cuid\")\n",
    "                \n",
    "#                 for \n",
    "                # Create Meta data directory\n",
    "#                 unique_meta_ids = imdfs.f\n",
    "                \n",
    "                imdfs.to_csv(os.path.join(meta_datas_frame, f'metadata.csv'))\n",
    "#                 imdfs.to_parquet(os.path.join(meta_datas_frame, f'metadata.parquet'))\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = '/data/data2/dev/af3c9242-b676-499a-8910-65addbddb8da.parquet'\n",
    "# df = pd.read_parquet(training_file)\n",
    "\n",
    "s = NSMain(vits16)\n",
    "s.process_all_frame(training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b39907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2d419",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf91aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ss = pd.concat(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe37df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.to_parquet('/data/data2/af3c9242-b676-499a-8910-65addbddb8da.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb96960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c95887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_file = './inter.parquet'\n",
    "\n",
    "vdf = pd.read_parquet(vector_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c9731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquet_to_np(vdf):    \n",
    "    vpaths = vdf.fimage.tolist()\n",
    "    ss = [(x1, y1, x2, y2) for (x1, y1, x2, y2) in zip(vdf.x1.tolist(), vdf.y1.tolist(), vdf.x2.tolist(), vdf.y2.tolist())]\n",
    "    vlatents = vdf.blkzero.tolist()\n",
    "    vlatents = np.array(vlatents)\n",
    "    \n",
    "    return vlatents, vpaths, ss\n",
    "\n",
    "def get_distance(search_lv, all_latents, all_paths, all_cc, count=64, dist='euclidean'):\n",
    "    d = distance.cdist(search_lv, all_latents, dist)\n",
    "\n",
    "    d = d.flatten()\n",
    "    idx = np.argsort(d)\n",
    "    candid_names_indices = idx[:count]\n",
    "    coordinate_indices = idx[:count]\n",
    "    \n",
    "    candid_names = np.array(all_paths)[candid_names_indices]\n",
    "    coordinate_names = np.array(all_cc)[coordinate_indices]\n",
    "    \n",
    "    thisdist = np.array(d)[candid_names_indices]\n",
    "    thisv = list(all_latents[candid_names_indices])\n",
    "       \n",
    "    \n",
    "    pd.DataFrame({'path': candid_names, 'distance': thisdist, 'V': thisv}).to_parquet(f'{dist}-query.parquet')\n",
    "    \n",
    "    \n",
    "    return d, candid_names_indices, candid_names, coordinate_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a5c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "vlatents, vpaths, coordinates = parquet_to_np(vdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcf3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(384 in vdf.x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f8ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "\n",
    "test_imga = PIL.Image.open('/data/data2/datasets/video/17bbd414-4132-464a-bc1e-ab5dd66c7e8f/high/frame_1.jpg').convert(\"RGB\")\n",
    "test_imgb = test_imga.crop((384, 0, 640, 256))\n",
    "\n",
    "txf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "test_imgc = txf(test_imgb)\n",
    "test_imgd = test_imgc[np.newaxis, :, :, :]\n",
    "\n",
    "intermediate_output = vits16.get_intermediate_layers(test_imgd, 6)\n",
    "\n",
    "output = [x[:, 0] for x in intermediate_output]\n",
    "\n",
    "output = torch.cat(output, dim=-1)\n",
    "npoutput = output.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8046df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12., 12.))\n",
    "\n",
    "grid = ImageGrid(fig, 111,\n",
    "                 nrows_ncols=(1, 1),\n",
    "                 axes_pad=0.3,\n",
    "                 )\n",
    "for ax, idx in zip(grid, range(1)):\n",
    "#     ax.set_title(f'{ttd[ttci[idx]]:.4f}')\n",
    "    ax.imshow(test_imgb)\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttd, ttci, ttcn, ttcc = get_distance(npoutput, vlatents, vpaths, coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63416910",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttd[0:3])\n",
    "print(ttci[0:3])\n",
    "print(ttcn[0:3])\n",
    "print(ttcc[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3bfe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig1 = plt.figure(figsize=(12., 12.))\n",
    "\n",
    "grid1 = ImageGrid(fig1, 111,\n",
    "                 nrows_ncols=(8, 8),\n",
    "                 axes_pad=0.3,\n",
    "                 )\n",
    "\n",
    "for ax, idx in zip(grid1, range(64)):\n",
    "    img = PIL.Image.open(ttcn[idx])\n",
    "    cc = ttcc[idx]\n",
    "    s = img.crop(cc)\n",
    "    \n",
    "#     ax.set_title(f'{ttd[ttci[idx]]:.4f}')\n",
    "    ax.set_title(coordinates[idx])\n",
    "    ax.imshow(s)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3abd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
